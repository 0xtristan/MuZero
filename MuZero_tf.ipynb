{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "MuZero_tf.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tfrizza/MuZero/blob/master/MuZero_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLzVDKNOxRH-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7084faf2-57e8-4d04-c6e2-51af9c71a2fa"
      },
      "source": [
        "import argparse\n",
        "import gym\n",
        "from gym import wrappers\n",
        "import numpy as np\n",
        "import pdb\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import gc\n",
        "import cv2\n",
        "import pdb\n",
        "from tqdm import tqdm\n",
        "\n",
        "import collections\n",
        "import math\n",
        "import typing\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torch import optim\n",
        "# import torch.nn.functional as F\n",
        "# import torchvision.transforms as T\n",
        "# from torch.autograd import Variable\n",
        "# from torchsummary import summary\n",
        "# device = torch.device('cpu')\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prYhI8PsyRtm",
        "colab_type": "text"
      },
      "source": [
        "## TPU Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qgpL6hax0j2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "outputId": "2d17d605-ae07-4fc1-c902-991de7bd888c"
      },
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "# This is the TPU initialization code that has to be at the beginning.\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.experimental.TPUStrategy(resolver)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.68.109.106:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.68.109.106:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvznC_cvyd0F",
        "colab_type": "text"
      },
      "source": [
        "## Gym Env"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkMoyNHzxRID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ENVS = {\n",
        "    'breakout': 'Breakout-v0',\n",
        "    'pong': 'Pong-v0',\n",
        "}\n",
        "STATE_PREPFN = {\n",
        "    'breakout': lambda s: s[50:, :, :],\n",
        "    'pong': lambda s: s[50:, :, :],\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnO36eBkxRIF",
        "colab_type": "text"
      },
      "source": [
        "Env has:\n",
        "* observation\n",
        "* reward\n",
        "* done\n",
        "* info\n",
        "\n",
        "Actions are:\n",
        "['NOOP', 'FIRE', 'RIGHT', 'LEFT']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYux-vt4xRIG",
        "colab_type": "code",
        "outputId": "58877756-f4a1-4030-bf7c-9a8eb7a05e0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "env = gym.make(ENVS['breakout'])\n",
        "\n",
        "# set seed\n",
        "# env.seed(args.seed)\n",
        "# np.random.seed(args.seed)\n",
        "# torch.manual_seed(args.seed)\n",
        "\n",
        "# episode loop\n",
        "for i in range(1):\n",
        "    o = env.reset()\n",
        "    render = plt.imshow(env.render(mode='rgb_array'))\n",
        "    # rollout inner loop\n",
        "    for t in range(100):\n",
        "        # render code\n",
        "        render.set_data(env.render(mode='rgb_array')) # just update the data\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)\n",
        "        \n",
        "#         print(o.shape)\n",
        "        a = env.action_space.sample()\n",
        "#         print(a)\n",
        "        o, r, done, _ = env.step(a)\n",
        "#         print(r)\n",
        "        if done:\n",
        "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
        "            break\n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdFpMQ0jxRIK",
        "colab_type": "text"
      },
      "source": [
        "## Helpers "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1wW58E0xRIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAXIMUM_FLOAT_VALUE = float('inf')\n",
        "\n",
        "KnownBounds = collections.namedtuple('KnownBounds', ['min', 'max'])\n",
        "\n",
        "\n",
        "class MinMaxStats(object):\n",
        "    \"\"\"A class that holds the min-max values of the tree.\"\"\"\n",
        "\n",
        "    def __init__(self, known_bounds: Optional[KnownBounds]):\n",
        "        self.maximum = known_bounds.max if known_bounds else -MAXIMUM_FLOAT_VALUE\n",
        "        self.minimum = known_bounds.min if known_bounds else MAXIMUM_FLOAT_VALUE\n",
        "\n",
        "    def update(self, value: float):\n",
        "        self.maximum = max(self.maximum, value)\n",
        "        self.minimum = min(self.minimum, value)\n",
        "\n",
        "    def normalize(self, value: float) -> float:\n",
        "        if self.maximum > self.minimum:\n",
        "            # We normalize only when we have set the maximum and minimum values.\n",
        "            return (value - self.minimum) / (self.maximum - self.minimum)\n",
        "        return value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFaqZtJBxRIN",
        "colab_type": "text"
      },
      "source": [
        "### MuZero Config Params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "excWi-RwxRIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MuZeroConfig(object):\n",
        "\n",
        "    def __init__(self,\n",
        "                action_space_size: int,\n",
        "                max_moves: int,\n",
        "                discount: float,\n",
        "                dirichlet_alpha: float,\n",
        "                num_simulations: int,\n",
        "                batch_size: int,\n",
        "                td_steps: int,\n",
        "                num_actors: int,\n",
        "                lr_init: float,\n",
        "                lr_decay_steps: float,\n",
        "                visit_softmax_temperature_fn,\n",
        "                known_bounds: Optional[KnownBounds] = None):\n",
        "        ### Self-Play\n",
        "        self.action_space_size = action_space_size\n",
        "        self.num_actors = num_actors\n",
        "\n",
        "        self.visit_softmax_temperature_fn = visit_softmax_temperature_fn\n",
        "        self.max_moves = max_moves\n",
        "        self.num_simulations = num_simulations\n",
        "        self.discount = discount\n",
        "\n",
        "        # Root prior exploration noise.\n",
        "        self.root_dirichlet_alpha = dirichlet_alpha\n",
        "        self.root_exploration_fraction = 0.25\n",
        "\n",
        "        # UCB formula\n",
        "        self.pb_c_base = 19652\n",
        "        self.pb_c_init = 1.25\n",
        "\n",
        "        # If we already have some information about which values occur in the\n",
        "        # environment, we can use them to initialize the rescaling.\n",
        "        # This is not strictly necessary, but establishes identical behaviour to\n",
        "        # AlphaZero in board games.\n",
        "        self.known_bounds = known_bounds\n",
        "\n",
        "        ### Training\n",
        "        self.selfplay_iterations = int(1e4) ##\n",
        "        self.training_steps = int(1000e3)\n",
        "        self.checkpoint_interval = int(1e3)\n",
        "        self.window_size = int(1e6)\n",
        "        self.batch_size = batch_size\n",
        "        self.num_unroll_steps = 5\n",
        "        self.td_steps = td_steps\n",
        "\n",
        "        self.weight_decay = 1e-4\n",
        "        self.momentum = 0.9\n",
        "\n",
        "        # Exponential learning rate schedule\n",
        "        self.lr_init = lr_init\n",
        "        self.lr_decay_rate = 0.1\n",
        "        self.lr_decay_steps = lr_decay_steps\n",
        "\n",
        "    def new_game(self):\n",
        "        return Game(self.action_space_size, self.discount)\n",
        "    \n",
        "def make_atari_config() -> MuZeroConfig:\n",
        "\n",
        "    def visit_softmax_temperature(num_moves, training_steps):\n",
        "        if training_steps < 500e3:\n",
        "            return 1.0\n",
        "        elif training_steps < 750e3:\n",
        "            return 0.5\n",
        "        else:\n",
        "            return 0.25\n",
        "\n",
        "    return MuZeroConfig(\n",
        "        action_space_size=4,\n",
        "        max_moves=27000,  # Half an hour at action repeat 4.\n",
        "        discount=0.997,\n",
        "        dirichlet_alpha=0.25,\n",
        "        num_simulations=50,\n",
        "        batch_size=1024,\n",
        "        td_steps=10,\n",
        "        num_actors=350,\n",
        "        lr_init=0.05,\n",
        "        lr_decay_steps=350e3,\n",
        "        visit_softmax_temperature_fn=visit_softmax_temperature)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSbIRX3NxRIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Action(object):\n",
        "\n",
        "    def __init__(self, index: int):\n",
        "        self.index = index\n",
        "\n",
        "    def __hash__(self):\n",
        "        return self.index\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return self.index == other.index\n",
        "\n",
        "    def __gt__(self, other):\n",
        "        return self.index > other.index\n",
        "    \n",
        "    def __str__(self):\n",
        "        return f\"Action({self.index})\"\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"<%s(%s) at %s>\" % (self.__class__.__name__, self.index, id(self))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nesCzurBxRIT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Node(object):\n",
        "\n",
        "    def __init__(self, prior: float):\n",
        "        self.visit_count = 0\n",
        "        self.to_play = -1\n",
        "        self.prior = prior\n",
        "        self.value_sum = 0\n",
        "        self.children = {}\n",
        "        self.hidden_state = None\n",
        "        self.reward = 0\n",
        "\n",
        "    def expanded(self) -> bool:\n",
        "        return len(self.children) > 0\n",
        "\n",
        "    def value(self) -> float:\n",
        "        if self.visit_count == 0:\n",
        "            return 0\n",
        "        return self.value_sum / self.visit_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHHtQR2NxRIV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ActionHistory(object):\n",
        "    \"\"\"Simple history container used inside the search.\n",
        "\n",
        "    Only used to keep track of the actions executed.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, history: List[Action], action_space_size: int):\n",
        "        self.history = list(history)\n",
        "        self.action_space_size = action_space_size\n",
        "\n",
        "    def clone(self):\n",
        "        return ActionHistory(self.history, self.action_space_size)\n",
        "\n",
        "    def add_action(self, action: Action):\n",
        "        self.history.append(action)\n",
        "\n",
        "    def last_action(self) -> Action:\n",
        "        return self.history[-1]\n",
        "\n",
        "    def action_space(self) -> List[Action]:\n",
        "        return [Action(i) for i in range(self.action_space_size)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNRzywV0xRIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Environment(object):\n",
        "    \"\"\"The environment MuZero is interacting with.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(ENVS['breakout'])\n",
        "        self.obs_history = [self.prepro(self.env.reset())]\n",
        "        self.done = False\n",
        "        \n",
        "    def step(self, action: Action):\n",
        "        obs, reward, self.done, info = self.env.step(action.__hash__())\n",
        "        self.obs_history.append(self.prepro(obs))\n",
        "        return reward\n",
        "    \n",
        "    def terminal(self):\n",
        "        return self.done\n",
        "    \n",
        "    def legal_actions(self):\n",
        "        \"\"\"Env specific rules for legality of moves\n",
        "        TODO: if at wall don't allow movement into the wall\"\"\"\n",
        "        return [Action(a) for a in range(self.env.action_space.n)]\n",
        "    \n",
        "    def prepro(self, obs, size=(80,80)):\n",
        "        \"\"\"Crop, resize, B&W\"\"\"\n",
        "        p_obs = obs[25:195,:,0] / 255 # crop and normalise to [0,1]\n",
        "        return cv2.resize(p_obs, size, interpolation=cv2.INTER_NEAREST) # resize\n",
        "                            \n",
        "    def get_obs(self, start:int, end:int=None):\n",
        "        return self.obs_history[max(start,0):end]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq_M9MvhxRIa",
        "colab_type": "text"
      },
      "source": [
        "![image.png](attachment:2ad59242-a802-4208-90ae-20ffe94ba7c8.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-stmlcz9xRIa",
        "colab_type": "text"
      },
      "source": [
        "Refresher on TD Learning, which is what is used to estimate the value targets.\n",
        "\n",
        "If we were to calculate value function without estimation we would have to wait until the final reward updates could be propagated:\n",
        "$$V(s_t) = V(s_t) + \\alpha[R_t-V(s_t)]$$\n",
        "where $s_t$ is state visited at time $t$, $R_t$ is total reward after time $t$ and $\\alpha$ is the LR.\n",
        "\n",
        "However for TD methods, an estimate of the final reward is calculated at each state and (s,a) value updated for every step of the way. We are essentially doing a finite step lookahead and updating based on that estimate. TD method is called a \"bootstrapping\" method, becuase the value is updated partly using an existing estimate and not a final reward.\n",
        "$$V(s_t) = V(s_t) + \\alpha[r_{t+1}+\\gamma V(s_{t+1})-V(s_t)]$$\n",
        "and in our case we formulate this as an N step bootstrap:\n",
        "$$z_t=u_{t+1} + \\gamma u_{t+2} + ... + \\gamma^{n-1} u_{t+n} + \\gamma^n \\nu_{t+n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JajZqo7xRIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Game(object):\n",
        "    \"\"\"A single episode of interaction with the environment. (One trajectory)\"\"\"\n",
        "    def __init__(self, action_space_size: int, discount: float):\n",
        "        self.env = Environment()\n",
        "        self.history = [] # actual actions a\n",
        "        self.rewards = [] # observed rewards u\n",
        "        self.child_visits = [] # search tree action distributions pi\n",
        "        self.root_values = [] # values ν\n",
        "        self.action_space_size = action_space_size\n",
        "        self.gamma = discount\n",
        "    \n",
        "    def terminal(self) -> bool:\n",
        "        return self.env.terminal()\n",
        "    \n",
        "    def legal_actions(self) -> List[Action]:\n",
        "        return self.env.legal_actions()\n",
        "    \n",
        "    def apply(self, action: Action):\n",
        "        reward = self.env.step(action)\n",
        "        self.rewards.append(reward)\n",
        "        self.history.append(action)\n",
        "        \n",
        "    def store_search_statistics(self, root: Node):\n",
        "        \"\"\"Stores the MCTS search value of node and search policy (visits ratio of children)\"\"\"\n",
        "        sum_visits = sum(child.visit_count for child in root.children.values())\n",
        "        action_space = (Action(index) for index in range(self.action_space_size))\n",
        "        # search policy π = [0.1,0.5,0.4] probability over children\n",
        "        self.child_visits.append([\n",
        "            root.children[a].visit_count / sum_visits if a in root.children else 0\n",
        "            for a in action_space\n",
        "        ])\n",
        "        # search value ν\n",
        "        self.root_values.append(root.value())\n",
        "    \n",
        "    def make_image(self, t: int, feat_history_len:int = 32):\n",
        "        \"\"\"Observation at chosen position w/ history\"\"\"\n",
        "        # Game specific feature planes\n",
        "        # For Atari we have the 32 most recent RGB frames at resolution 96x96\n",
        "        # Instead I use 80x80x1 B&W\n",
        "        frames = self.env.get_obs(t-feat_history_len+1, t+2) # We want 32 frames up to and including t\n",
        "        # Cast to tensor and add dummy batch dim\n",
        "        # Todo: figure out how to stack RGB images - i.e. colour & time dimensions\n",
        "        frame_tensor = tf.convert_to_tensor(np.stack(frames,axis=-1))\n",
        "        # If we're missing a channel dim add one\n",
        "        # if len(frame_tensor.shape)==3:\n",
        "        #     frame_tensor = frame_tensor.expand_dims(-1) # this is wrong\n",
        "        # Pad out sequences with insufficient history\n",
        "        padding_size = feat_history_len-frame_tensor.shape[-1]\n",
        "        padded_frames = tf.pad(frame_tensor, paddings=[[0, 0], [0, 0], [padding_size, 0]], constant_values=0)\n",
        "        padded_frames = tf.expand_dims(padded_frames, 0) # dummy batch dim for 4D\n",
        "        return padded_frames\n",
        "    \n",
        "    def make_target(self, t: int, K: int, td: int):\n",
        "        \"\"\"\n",
        "        (value,reward,policy) target for each unroll step t to t+K\n",
        "        This is taken from actuals results of the game (to be stored in replay buffer)\n",
        "        Uses TD learning to calculate value target via n step bootstrap, see above\n",
        "        \"\"\"\n",
        "        # The value target is the discounted root value of the search tree N steps\n",
        "        # into the future, plus the discounted sum of all rewards until then.\n",
        "        # Returns target tuple (value, reward, policy) i.e. (z,u,pi)\n",
        "        # K=5\n",
        "        targets = []\n",
        "        for current_index in range(t, t + K + 1):\n",
        "            ## Value Target z_{t+K} ##\n",
        "            \n",
        "            bootstrap_index = current_index + td \n",
        "            # If our TD lookahead is still before the end of the game, the update with that \n",
        "            # future game state value estimate ν_{t+N}\n",
        "            if bootstrap_index < len(self.root_values):\n",
        "                # γ^N*ν_{t+N}\n",
        "                value =  self.root_values[bootstrap_index] * self.gamma**td\n",
        "            else:\n",
        "                value = 0\n",
        "            \n",
        "            # Rest of the TD value estimate from observed rewards: u_{t+1} + ... + γ^{N-1} u_{t+n}\n",
        "            for i, reward in enumerate(self.rewards[current_index:bootstrap_index]):\n",
        "                value += reward * self.gamma**i\n",
        "                \n",
        "            ## Reward u_{t+K} and Action π_{t+K} Targets ##\n",
        "            \n",
        "            # For simplicity the network always predicts the most recently received\n",
        "            # reward, even for the initial representation network where we already\n",
        "            # know this reward.\n",
        "            if current_index > 0 and current_index <= len(self.rewards):\n",
        "                last_reward = self.rewards[current_index - 1]\n",
        "            else:\n",
        "                last_reward = 0\n",
        "            \n",
        "            if current_index < len(self.root_values):\n",
        "                targets.append((value, last_reward, self.child_visits[current_index]))\n",
        "            else:\n",
        "                # States past the end are treated as absorbing states\n",
        "                targets.append((0, last_reward, []))\n",
        "        return targets \n",
        "    \n",
        "    def action_history(self) -> ActionHistory:\n",
        "        return ActionHistory(self.history, self.action_space_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc3EWn8gxRId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "    \"\"\"Stores the target tuples to sample from later during training\"\"\"\n",
        "    def __init__(self, config: MuZeroConfig):\n",
        "        self.window_size = config.window_size\n",
        "        self.batch_size = config.batch_size\n",
        "        self.buffer = []\n",
        "\n",
        "    def save_game(self, game):\n",
        "        # Pop off oldest replays to make space for new ones\n",
        "        if len(self.buffer) > self.window_size:\n",
        "            self.buffer.pop(0)\n",
        "        self.buffer.append(game)\n",
        "\n",
        "    def sample_batch(self, K: int, td: int):\n",
        "        \"\"\"\n",
        "        Inputs\n",
        "            K: num unroll steps\n",
        "            td: num TD steps\n",
        "        Outputs\n",
        "            (observation, next K actions, target tuple)\n",
        "        \"\"\"\n",
        "        games = [self.sample_game() for _ in range(self.batch_size)]\n",
        "        game_pos = [(g, self.sample_position(g)) for g in games]\n",
        "        return [(g.make_image(i), g.history[i:i + K],\n",
        "                 g.make_target(i, K, td))\n",
        "                for (g, i) in game_pos]\n",
        "\n",
        "    def sample_game(self) -> Game:\n",
        "        # TODO: figure out sampling regime\n",
        "        # Sample game from buffer either uniformly or according to some priority e.g. importance sampling.\n",
        "        game_ix = random.randint(0,len(self.buffer)-1) # random uniform\n",
        "        return self.buffer[game_ix]\n",
        "\n",
        "    def sample_position(self, game) -> int:\n",
        "        # Sample position from game either uniformly or according to some priority.\n",
        "        pos_ix = random.randint(0,len(game.root_values)-1) # random uniform\n",
        "        return pos_ix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0pD-gdHxRIg",
        "colab_type": "text"
      },
      "source": [
        "## NN Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgNoaX2qxRIh",
        "colab_type": "text"
      },
      "source": [
        "#### 1. Prediction Network (F)\n",
        "The prediction function pk, vk = fθ(sk) uses the same architecture as AlphaZero: one or two convolutional layers that preserve the resolution but reduce the number of planes, followed by a fully connected layer to the size of the output.\n",
        "\n",
        "#### 2. Dynamics Network (G)\n",
        "Both the representation and dynamics function use the same architecture as AlphaZero, but with 16 instead of 20 residual blocks. We use 3x3 kernels and 256 hidden planes for each convolution. \n",
        "\n",
        "For the dynamics function (which always operates at the downsampled resolution of 6x6), the action is first encoded as an image, then stacked with the hidden state of the previous step along the plane dimension\n",
        "\n",
        "#### 3. Representation Network (H)\n",
        "For Atari, where observations have large spatial resolution, the representation function starts with a sequence\n",
        "of convolutions with stride 2 to reduce the spatial resolution. Specifically, starting with an input observation of resolution 96x96 and 128 planes (32 history frames of 3 color channels each, concatenated with the corresponding 32 actions broadcast to planes), we downsample as follows:\n",
        "* 1 convolution with stride 2 and 128 output planes, output resolution 48x48. \n",
        "* 2 residual blocks with 128 planes \n",
        "* 1 convolution with stride 2 and 256 output planes, output resolution 24x24. \n",
        "* 3 residual blocks with 256 planes. \n",
        "* Average pooling with stride 2, output resolution 12x12. \n",
        "* 3 residual blocks with 256 planes. \n",
        "* Average pooling with stride 2, output resolution 6x6.\n",
        "The kernel size is 3x3 for all operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZYPap-uzHMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Dense, Add, ReLU, Input, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def ResBlock(x_in, nf=128):\n",
        "    x = Conv2D(nf, 3, padding='same', use_bias=False)(x_in)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = Conv2D(nf, 3, padding='same', use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Add()([x, input_data])\n",
        "    x = ReLU()(x)\n",
        "    return x\n",
        "\n",
        "def ConvBlock(x_in, nf, s=1, bn=True):\n",
        "    x = Conv2D(nf, 3, padding='same', strides=s, use_bias=not bn)(x_in)\n",
        "    if bn: x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    return x\n",
        "\n",
        "def ReprNet(input_shape=(80,80,32), nf=128):\n",
        "    o = Input(shape=input_shape)\n",
        "    x = ConvBlock(o, nf, 2)\n",
        "    x = ConvBlock(x, nf, 2)\n",
        "    x = ConvBlock(x, nf, 2)\n",
        "    s = ConvBlock(x, nf, 2)\n",
        "    return Model(o, s)\n",
        "\n",
        "def DynaNet(input_shape=(5,5,129), nf=128):\n",
        "    s = Input(shape=input_shape)\n",
        "    x = ConvBlock(s, nf)\n",
        "    x = ConvBlock(x, nf)\n",
        "    x = ConvBlock(x, nf)\n",
        "    s_new = ConvBlock(x, nf)\n",
        "    \n",
        "    r = Flatten()(s_new)\n",
        "    r = Dense(1)(r)\n",
        "    return Model(s, [s_new, r])\n",
        "\n",
        "def PredNet(input_shape=(5,5,128), nf=64, num_actions=4):\n",
        "    s = Input(shape=input_shape)\n",
        "    x = ConvBlock(s, nf)\n",
        "    x = ConvBlock(x, nf//2) \n",
        "    x = Flatten()(x)\n",
        "\n",
        "    a = Dense(num_actions)(x)\n",
        "    v = Dense(1)(x)\n",
        "    return Model(s, [a, v])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hOyHqx-xRIj",
        "colab_type": "code",
        "outputId": "aa33a15d-4aba-4391-dbc0-fddc44ba9fd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        }
      },
      "source": [
        "# Input: (32,80,80) - 80x80 game in B&W with 32 length obs history\n",
        "# Output: (128,5,5) - s0\n",
        "ReprNet().summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         [(None, 80, 80, 32)]      0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 40, 40, 128)       36864     \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 40, 40, 128)       512       \n",
            "_________________________________________________________________\n",
            "re_lu_15 (ReLU)              (None, 40, 40, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 20, 20, 128)       147456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 20, 20, 128)       512       \n",
            "_________________________________________________________________\n",
            "re_lu_16 (ReLU)              (None, 20, 20, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 10, 10, 128)       147456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 10, 10, 128)       512       \n",
            "_________________________________________________________________\n",
            "re_lu_17 (ReLU)              (None, 10, 10, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 5, 5, 128)         147456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 5, 5, 128)         512       \n",
            "_________________________________________________________________\n",
            "re_lu_18 (ReLU)              (None, 5, 5, 128)         0         \n",
            "=================================================================\n",
            "Total params: 481,280\n",
            "Trainable params: 480,256\n",
            "Non-trainable params: 1,024\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVUBy_d8xRIm",
        "colab_type": "code",
        "outputId": "3a65192a-92c8-4480-d2fc-d320e2e35249",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        }
      },
      "source": [
        "# Input: (129,5,5) - 5x5 internal state with nf channels +1 action dim\n",
        "# Output: (128,5,5), (1,) - s', r\n",
        "DynaNet().summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         [(None, 5, 5, 129)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 5, 5, 128)         148608    \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 5, 5, 128)         512       \n",
            "_________________________________________________________________\n",
            "re_lu_19 (ReLU)              (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 5, 5, 128)         147456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 5, 5, 128)         512       \n",
            "_________________________________________________________________\n",
            "re_lu_20 (ReLU)              (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 5, 5, 128)         147456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 5, 5, 128)         512       \n",
            "_________________________________________________________________\n",
            "re_lu_21 (ReLU)              (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 5, 5, 128)         147456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 5, 5, 128)         512       \n",
            "_________________________________________________________________\n",
            "re_lu_22 (ReLU)              (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 3200)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 3201      \n",
            "=================================================================\n",
            "Total params: 596,225\n",
            "Trainable params: 595,201\n",
            "Non-trainable params: 1,024\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5TKxIctxRIo",
        "colab_type": "code",
        "outputId": "42295954-0d73-41f7-bffa-3323d853ce68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        }
      },
      "source": [
        "# Input: (128,5,5) - 5x5 internal state with nf channels\n",
        "# Output: (4,), (1,) - p, v\n",
        "PredNet().summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_10 (InputLayer)           [(None, 5, 5, 128)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 5, 5, 64)     73728       input_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 5, 5, 64)     256         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_23 (ReLU)                 (None, 5, 5, 64)     0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 5, 5, 32)     18432       re_lu_23[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 5, 5, 32)     128         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_24 (ReLU)                 (None, 5, 5, 32)     0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 800)          0           re_lu_24[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 4)            3204        flatten_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            801         flatten_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 96,549\n",
            "Trainable params: 96,357\n",
            "Non-trainable params: 192\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiHSAcpFxRIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NetworkOutput(typing.NamedTuple):\n",
        "    value: float\n",
        "    reward: float\n",
        "    policy_logits: Dict[Action, float]\n",
        "    hidden_state: List[float] # not sure about this one lol"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35S6OKqbxRIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Network(object):\n",
        "    \"\"\"TODO: Implement in Pytorch/TF\"\"\"\n",
        "    def __init__(self, h_in=5, w_in=5, c_in=32, nf=128, n_acts=4):\n",
        "        # Initialise a uniform network - should I init these networks explicitly?\n",
        "        self.f = PredNet((5,5,nf), nf, n_acts)\n",
        "        self.g = DynaNet((5,5,nf+1), nf)\n",
        "        self.h = ReprNet((80,80,32), nf)\n",
        "        self.steps = 0\n",
        "        \n",
        "    def initial_inference(self, obs) -> NetworkOutput:\n",
        "        # representation + prediction function\n",
        "        # input: 32x80x80 observation\n",
        "        state = self.h(obs)\n",
        "        policy_logits, value = self.f(state)\n",
        "        policy = {Action(i):p for i,p in enumerate(policy_logits[0].numpy())}\n",
        "        return NetworkOutput(value[0], 0, policy, state) # keep state 4D\n",
        "\n",
        "    def recurrent_inference(self, state, action) -> NetworkOutput:\n",
        "        # dynamics + prediction function\n",
        "        # Input: hidden state nfx5x5\n",
        "        # Concat/pad action to channel dim of states\n",
        "        state_action = tf.pad(state, paddings=[[0, 0], [0, 0], [0, 0], [0, 1]], constant_values=action.__hash__())\n",
        "        next_state, reward =  self.g(state_action)\n",
        "        policy_logits, value = self.f(next_state)\n",
        "        policy = {Action(i):p for i,p in enumerate(policy_logits[0].numpy())}\n",
        "        return NetworkOutput(value[0], reward[0], policy, next_state)\n",
        "\n",
        "    def get_weights(self):\n",
        "        # Returns the weights of this network.\n",
        "        self.steps += 1 # probably not ideal\n",
        "        return [self.f.parameters(), self.g.parameters(), self.h.parameters()]\n",
        "\n",
        "    def training_steps(self) -> int:\n",
        "        # How many steps / batches the network has been trained for.\n",
        "        return self.steps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNRwMH0AxRIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SharedStorage(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self._networks = {}\n",
        "\n",
        "    def latest_network(self) -> Network:\n",
        "        if self._networks:\n",
        "            return self._networks[max(self._networks.keys())]\n",
        "        else:\n",
        "            # policy -> uniform, value -> 0, reward -> 0\n",
        "            return make_uniform_network()\n",
        "\n",
        "    def save_network(self, step: int, network: Network):\n",
        "        self._networks[step] = network\n",
        "\n",
        "def make_uniform_network():\n",
        "    return Network()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPaehAh9xRIx",
        "colab_type": "text"
      },
      "source": [
        "# 1. Self-play"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5MiJYXOxRIx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Each self-play job is independent of all others; it takes the latest network\n",
        "# snapshot, produces a game and makes it available to the training job by\n",
        "# writing it to a shared replay buffer.\n",
        "def run_selfplay(config: MuZeroConfig, storage: SharedStorage,\n",
        "                 replay_buffer: ReplayBuffer):\n",
        "    \"\"\"TODO: Multithread this\"\"\"\n",
        "    for i in tqdm(range(config.selfplay_iterations), desc='Self-play iter'):\n",
        "        network = storage.latest_network()\n",
        "        game = play_game(config, network)\n",
        "        replay_buffer.save_game(game)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ijhRSlVxRIz",
        "colab_type": "text"
      },
      "source": [
        "### Run 1 Game/Trajectory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSB8CTSwxRIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Each game is produced by starting at the initial board position, then\n",
        "# repeatedly executing a Monte Carlo Tree Search to generate moves until the end\n",
        "# of the game is reached.\n",
        "def play_game(config: MuZeroConfig, network: Network) -> Game:\n",
        "    game = config.new_game()\n",
        "\n",
        "    while not game.terminal() and len(game.history) < config.max_moves:\n",
        "        # At the root of the search tree we use the representation function h to\n",
        "        # obtain a hidden state given the current observation.\n",
        "        root = Node(0)\n",
        "        current_observation = game.make_image(-1) # 80x80x32\n",
        "        expand_node(root, game.legal_actions(),\n",
        "                    network.initial_inference(current_observation))\n",
        "        add_exploration_noise(config, root)\n",
        "\n",
        "        # We then run a Monte Carlo Tree Search using only action sequences and the\n",
        "        # model learned by the network.\n",
        "        run_mcts(config, root, game.action_history(), network)\n",
        "        action = select_action(config, len(game.history), root, network)\n",
        "        game.apply(action)\n",
        "        game.store_search_statistics(root)\n",
        "    return game"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbsyVFyRxRI1",
        "colab_type": "text"
      },
      "source": [
        "#### Exploration Noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AegOB2i0xRI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# At the start of each search, we add dirichlet noise to the prior of the root\n",
        "# to encourage the search to explore new actions.\n",
        "def add_exploration_noise(config: MuZeroConfig, node: Node):\n",
        "    actions = list(node.children.keys())\n",
        "    noise = np.random.dirichlet([config.root_dirichlet_alpha] * len(actions))\n",
        "    frac = config.root_exploration_fraction\n",
        "    for a, n in zip(actions, noise):\n",
        "        node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvObjpTlxRI3",
        "colab_type": "text"
      },
      "source": [
        "### Softmax search policy $\\pi$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HYHEl9WxRI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def select_action(config: MuZeroConfig, num_moves: int, node: Node,\n",
        "                  network: Network) -> Action:\n",
        "    \"\"\"Search policy: softmax probability over actions dictated by visited counts\"\"\"\n",
        "    # Visit counts of chilren nodes - policy proportional to counts\n",
        "    visit_counts = [\n",
        "        (child.visit_count, action) for action, child in node.children.items()\n",
        "    ]\n",
        "    # Get softmax temp\n",
        "    t = config.visit_softmax_temperature_fn(\n",
        "        num_moves=num_moves, training_steps=network.training_steps())\n",
        "    _, action = softmax_sample(visit_counts, t)\n",
        "    return action\n",
        "\n",
        "def softmax_sample(distribution, T: float):\n",
        "    counts = np.array([d[0] for d in distribution])\n",
        "    actions = [d[1] for d in distribution]\n",
        "    softmax_probs = softmax(counts/T)\n",
        "    sampled_action = np.random.choice(actions, size=1, p=softmax_probs)[0]\n",
        "    return _, sampled_action\n",
        "#     return nn.Softmax(dim=0)(distribution/T)\n",
        "\n",
        "def softmax(x, axis=-1):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=axis, keepdims=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb7XvbKBxRI6",
        "colab_type": "text"
      },
      "source": [
        "### MCTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB69k9kHxRI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Core Monte Carlo Tree Search algorithm.\n",
        "# To decide on an action, we run N simulations, always starting at the root of\n",
        "# the search tree and traversing the tree according to the UCB formula until we\n",
        "# reach a leaf node.\n",
        "def run_mcts(config: MuZeroConfig, root: Node, action_history: ActionHistory,\n",
        "             network: Network):\n",
        "    \"\"\"TODO: Multithread\"\"\"\n",
        "    min_max_stats = MinMaxStats(config.known_bounds)\n",
        "\n",
        "    for _ in range(config.num_simulations):\n",
        "        history = action_history.clone()\n",
        "        node = root\n",
        "        search_path = [node]\n",
        "\n",
        "        # Traverse tree, expanding by highest UCB until leaf reached\n",
        "        while node.expanded():\n",
        "            action, node = select_child(config, node, min_max_stats) # UCB selection\n",
        "            history.add_action(action)\n",
        "            search_path.append(node)\n",
        "\n",
        "        # Inside the search tree we use the dynamics function to obtain the next\n",
        "        # hidden state given an action and the previous hidden state.\n",
        "        parent = search_path[-2] # parent of leaf\n",
        "        # Dynamics: g(s_{k-1},a_k) = r_k, s_k\n",
        "        # Predictions: f(s_k) = v_k, p_k\n",
        "        # -> (v,r,p,s)\n",
        "        network_output = network.recurrent_inference(parent.hidden_state,\n",
        "                                                     history.last_action())\n",
        "        # expand node using v,r,p predictions from NN\n",
        "        expand_node(node, history.action_space(), network_output)\n",
        "\n",
        "        # back up values to the root node\n",
        "        backpropagate(search_path, network_output.value, config.discount, \n",
        "                      min_max_stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FUTdI0axRI9",
        "colab_type": "text"
      },
      "source": [
        "#### i. Selection: UCB Child Selection\n",
        "We select the child node/state based on the action that maximises over an upper confidence bound (UCB):\n",
        "$$a^{k}=\\arg \\max _{a}\\left[Q(s, a)+P(s, a) \\cdot \\frac{\\sqrt{\\sum_{b} N(s, b)}}{1+N(s, a)}\\left(c_{1}+\\log \\left(\\frac{\\sum_{b} N(s, b)+c_{2}+1}{c_{2}}\\right)\\right)\\right]$$\n",
        "To clarify: $\\sum_{b} N(s, b)$ is just the sum of counts over all child nodes, i.e. the count of the parent node.\n",
        "\n",
        "\"The constants c1 and c2 are used to control the influence of the prior P(s, a) relative to the value Q(s, a) as\n",
        "nodes are visited more often. In our experiments, $c_1$ = 1.25 and $c_2$ = 19652\" - (pg. 12)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyKU0IngxRI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select the child with the highest UCB score.\n",
        "def select_child(config: MuZeroConfig, node: Node,\n",
        "                 min_max_stats: MinMaxStats):\n",
        "    _, action, child = max(\n",
        "        (ucb_score(config, node, child, min_max_stats), action,\n",
        "         child) for action, child in node.children.items())\n",
        "    return action, child\n",
        "\n",
        "# The score for a node is based on its value, plus an exploration bonus based on\n",
        "# the prior.\n",
        "def ucb_score(config: MuZeroConfig, parent: Node, child: Node,\n",
        "              min_max_stats: MinMaxStats) -> float:\n",
        "    pb_c = math.log((parent.visit_count + config.pb_c_base + 1) /\n",
        "                  config.pb_c_base) + config.pb_c_init\n",
        "    pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
        "\n",
        "    # P(s,a)*pb_c\n",
        "    prior_score = pb_c * child.prior\n",
        "    # Q(s,a)\n",
        "    if child.visit_count > 0:\n",
        "        value_score = child.reward + config.discount * min_max_stats.normalize(\n",
        "            child.value())\n",
        "    else:\n",
        "        value_score = 0\n",
        "    return prior_score + value_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh_PczcTxRI_",
        "colab_type": "text"
      },
      "source": [
        "#### ii. Expansion: Leaf Node Expansion + Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r9K4PfZxRI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We expand a node using the value, reward and policy prediction obtained from\n",
        "# the neural network.\n",
        "def expand_node(node: Node, actions: List[Action], network_output: NetworkOutput):\n",
        "    \"\"\"Updates predictions for state s, reward r and policy p for node based on NN outputs\"\"\"\n",
        "    # Update leaf with predictions from parent\n",
        "    node.hidden_state = network_output.hidden_state # s\n",
        "    node.reward = network_output.reward # r\n",
        "    policy = {a: math.exp(network_output.policy_logits[a]) for a in actions} # unnormalised probabilities\n",
        "    policy_sum = sum(policy.values()) \n",
        "    for action, p in policy.items():\n",
        "        node.children[action] = Node(p / policy_sum) # p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh-o6iq9xRJE",
        "colab_type": "text"
      },
      "source": [
        "#### iii. Backup: Search Tree Update/Backprop\n",
        "$$G^{k}=\\sum_{\\tau=0}^{l-1-k} \\gamma^{\\tau} r_{k+1+\\tau}+\\gamma^{l-k} v^{l}$$\n",
        "For $k = l...1$, we update the statistics for each edge $\\left(s^{k-1}, a^{k}\\right)$ in the simulation\n",
        "$$Q\\left(s^{k-1}, a^{k}\\right):=\\frac{N\\left(s^{k-1}, a^{k}\\right) \\cdot Q\\left(s^{k-1}, a^{k}\\right)+G^{k}}{N\\left(s^{k-1}, a^{k}\\right)+1}$$\n",
        "$$N\\left(s^{k-1}, a^{k}\\right):=N\\left(s^{k-1}, a^{k}\\right)+1$$\n",
        "Q value estimates are normalised so that we can combine value estimates with probabilities via pUCT rule (above)\n",
        "$$\\bar{Q}\\left(s^{k-1}, a^{k}\\right)=\\frac{Q\\left(s^{k-1}, a^{k}\\right)-\\min _{s, a \\in \\operatorname{Tree}} Q(s, a)}{\\max _{s, a \\in \\operatorname{Tree}} Q(s, a)-\\min _{s, a \\in \\operatorname{Tree}} Q(s, a)}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3VBP2LnxRJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# At the end of a simulation, we propagate the evaluation all the way up the\n",
        "# tree to the root.\n",
        "def backpropagate(search_path: List[Node], value: float,\n",
        "                  discount: float, min_max_stats: MinMaxStats):\n",
        "    # Traverse back up UCB search path\n",
        "    for node in reversed(search_path):\n",
        "        node.value_sum += value # if node.to_play == to_play else -value\n",
        "        node.visit_count += 1\n",
        "        min_max_stats.update(node.value())\n",
        "\n",
        "        value = node.reward + discount * value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W988LWPxRJG",
        "colab_type": "text"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ifGNTeLxRJH",
        "colab_type": "code",
        "outputId": "8fcf7e63-e57d-424d-d595-924c51bfeb7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        }
      },
      "source": [
        "with strategy.scope():\n",
        "    config = make_atari_config()\n",
        "    ss = SharedStorage()\n",
        "    rbuf = ReplayBuffer(config)\n",
        "\n",
        "    run_selfplay(config, ss, rbuf)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Self-play iter:   0%|          | 0/10000 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-e222fd90f218>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mrbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReplayBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mrun_selfplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-a34981bbf1c1>\u001b[0m in \u001b[0;36mrun_selfplay\u001b[0;34m(config, storage, replay_buffer)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselfplay_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Self-play iter'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-805f78aa4370>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(config, network)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# We then run a Monte Carlo Tree Search using only action sequences and the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# model learned by the network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mrun_mcts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-6a2401fc79df>\u001b[0m in \u001b[0;36mrun_mcts\u001b[0;34m(config, root, action_history, network)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# -> (v,r,p,s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         network_output = network.recurrent_inference(parent.hidden_state,\n\u001b[0;32m---> 24\u001b[0;31m                                                      history.last_action())\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;31m# expand node using v,r,p predictions from NN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mexpand_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-68-5f9cfbcb324d>\u001b[0m in \u001b[0;36mrecurrent_inference\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Concat/pad action to channel dim of states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mstate_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstant_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mpolicy_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    717\u001b[0m     return self._run_internal_graph(\n\u001b[1;32m    718\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    640\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m     \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m       flattened_dim = tensor_shape.dimension_value(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0;31m# `_tensor_shape` is declared and defined in the definition of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \u001b[0;31m# `EagerTensor`, in C.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEzfMX92xRJJ",
        "colab_type": "code",
        "outputId": "79d517bc-bc55-4a2d-f96b-125f077e5ecd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "pdb.pm()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> <ipython-input-36-ea3fb6154890>(15)initial_inference()\n",
            "-> policy = {Action(i):p for i,p in enumerate(policy_logits[0].tolist())}\n",
            "(Pdb) policy_logits\n",
            "<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
            "array([[ 0.00415338, -0.00234582,  0.01239257,  0.00425528]],\n",
            "      dtype=float32)>\n",
            "(Pdb) policy_logits[0].numpy()\n",
            "array([ 0.00415338, -0.00234582,  0.01239257,  0.00425528], dtype=float32)\n",
            "(Pdb) [i for i in policy_logits[0].numpy()]\n",
            "[0.004153385, -0.002345816, 0.012392567, 0.0042552846]\n",
            "(Pdb) q\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_jnSLpCxRJL",
        "colab_type": "text"
      },
      "source": [
        "#### Status\n",
        "Self play runs! Now to check if it's actually doing what we want it to..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSMjtIwExRJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}